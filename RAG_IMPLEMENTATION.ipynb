{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPrnrqEhwl7+PzOMOdKcYol",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MeenakshiRajpurohit/CMPE-252-AI-and-Data-Engineering/blob/main/RAG_IMPLEMENTATION.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install -q langchain langchain-core langchain-text-splitters langchain-community langchain-huggingface huggingface-hub sentence-transformers faiss-cpu transformers torch\n",
        "\n",
        "# Import libraries\n",
        "import os\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_huggingface import HuggingFaceEndpoint  # Use HuggingFaceEndpoint instead\n",
        "from google.colab import userdata\n",
        "\n",
        "# Set up API key from Colab secrets\n",
        "try:\n",
        "    huggingface_api_key = userdata.get('HUGGINGFACEHUB_API_TOKEN')\n",
        "except:\n",
        "    from getpass import getpass\n",
        "    huggingface_api_key = getpass('Enter your Hugging Face API token: ')\n",
        "\n",
        "# Set environment variable\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = huggingface_api_key\n",
        "\n",
        "# Create sample document\n",
        "os.makedirs('data', exist_ok=True)\n",
        "sample_text = \"\"\"\n",
        "Polar bears are facing significant threats due to climate change.\n",
        "The melting of Arctic sea ice is reducing their hunting grounds.\n",
        "Polar bears primarily hunt seals from sea ice platforms.\n",
        "Without adequate ice, polar bears struggle to find food.\n",
        "Scientists consider polar bears to be vulnerable to extinction.\n",
        "The loss of sea ice habitat is the primary danger to polar bears.\n",
        "Conservation efforts are underway to protect polar bear populations.\n",
        "\"\"\"\n",
        "\n",
        "with open('data/my_document.txt', 'w') as f:\n",
        "    f.write(sample_text)\n",
        "\n",
        "# Load the document\n",
        "loader = TextLoader('data/my_document.txt')\n",
        "documents = loader.load()\n",
        "print(f\"✓ Loaded {len(documents)} document(s)\")\n",
        "\n",
        "# Split the document into chunks\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "document_chunks = splitter.split_documents(documents)\n",
        "print(f\"✓ Split into {len(document_chunks)} chunks\")\n",
        "\n",
        "# Initialize embeddings\n",
        "print(\"Loading embeddings model...\")\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        ")\n",
        "print(\"✓ Embeddings loaded\")\n",
        "\n",
        "# Create FAISS vector store\n",
        "print(\"Creating vector store...\")\n",
        "vector_store = FAISS.from_documents(document_chunks, embeddings)\n",
        "print(\"✓ Vector store created\")\n",
        "\n",
        "# Get retriever\n",
        "retriever = vector_store.as_retriever()\n",
        "\n",
        "# Initialize the LLM using HuggingFaceEndpoint (more stable)\n",
        "print(\"Initializing LLM...\")\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"google/flan-t5-large\",\n",
        "    task=\"text2text-generation\",\n",
        "    huggingfacehub_api_token=huggingface_api_key, # Pass the API key directly\n",
        "    temperature=0.7, # Moved out of model_kwargs\n",
        "    max_new_tokens=512 # Changed from max_length to max_new_tokens\n",
        ")\n",
        "print(\"✓ LLM initialized\")\n",
        "\n",
        "# Create simple RAG function\n",
        "def ask_question(query):\n",
        "    \"\"\"Simple RAG function\"\"\"\n",
        "    # Retrieve relevant documents\n",
        "    relevant_docs = retriever.invoke(query)\n",
        "\n",
        "    # Combine context\n",
        "    context = \"\\n\".join([doc.page_content for doc in relevant_docs])\n",
        "\n",
        "    # Create prompt - simplified for T5 model\n",
        "    prompt = f\"Context: {context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
        "\n",
        "    # Get response\n",
        "    response = llm.invoke(prompt)\n",
        "\n",
        "    return response\n",
        "\n",
        "# Example query\n",
        "query = \"Are polar bears in danger?\"\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(f\"Query: {query}\")\n",
        "print(f\"{'='*50}\")\n",
        "print(\"Generating response...\\n\")\n",
        "\n",
        "try:\n",
        "    response = ask_question(query)\n",
        "\n",
        "    # Print response\n",
        "    print(\"=\"*50)\n",
        "    print(\"RESPONSE:\")\n",
        "    print(\"=\"*50)\n",
        "    print(response)\n",
        "except Exception as e:\n",
        "    print(f\"Error occurred: {e}\")\n",
        "    print(\"\\nTrying alternative approach...\")\n",
        "\n",
        "    # Fallback: Use a local pipeline\n",
        "    from transformers import pipeline\n",
        "\n",
        "    print(\"Loading local model...\")\n",
        "    qa_pipeline = pipeline(\n",
        "        \"text2text-generation\",\n",
        "        model=\"google/flan-t5-base\",\n",
        "        max_length=512\n",
        "    )\n",
        "\n",
        "    relevant_docs = retriever.invoke(query)\n",
        "    context = \"\\n\".join([doc.page_content for doc in relevant_docs])\n",
        "    prompt = f\"Context: {context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
        "\n",
        "    response = qa_pipeline(prompt)[0]['generated_text']\n",
        "\n",
        "    print(\"=\"*50)\n",
        "    print(\"RESPONSE:\")\n",
        "    print(\"=\"*50)\n",
        "    print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kszZ96NrMaI1",
        "outputId": "c9b244f2-fc97-4d8c-ace4-c4b6186d7095"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your Hugging Face API token: ··········\n",
            "✓ Loaded 1 document(s)\n",
            "✓ Split into 1 chunks\n",
            "Loading embeddings model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_huggingface.llms.huggingface_endpoint:WARNING! max_length is not default parameter.\n",
            "                    max_length was transferred to model_kwargs.\n",
            "                    Please make sure that max_length is what you intended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Embeddings loaded\n",
            "Creating vector store...\n",
            "✓ Vector store created\n",
            "Initializing LLM...\n",
            "✓ LLM initialized\n",
            "\n",
            "==================================================\n",
            "Query: Are polar bears in danger?\n",
            "==================================================\n",
            "Generating response...\n",
            "\n",
            "Error occurred: InferenceClient.text_generation() got an unexpected keyword argument 'max_length'\n",
            "\n",
            "Trying alternative approach...\n",
            "Loading local model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "RESPONSE:\n",
            "==================================================\n",
            "polar bears are facing significant threats due to climate change\n"
          ]
        }
      ]
    }
  ]
}