{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MeenakshiRajpurohit/CMPE-252-AI-and-Data-Engineering/blob/main/VQA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "#  Finance Domain-Specific Fine-Tuning of VQA Models\n",
        "#  Dataset: sujet-ai/Sujet-Finance-QA-Vision-100k\n",
        "#  Model:   Qwen/Qwen2-VL-2B-Instruct\n",
        "#  Runs on: Google Colab (T4 GPU)\n",
        "# ============================================================\n",
        "# HOW TO USE:\n",
        "#   1. New Colab notebook -> Runtime -> T4 GPU\n",
        "#   2. Run Section 2 first, then Runtime -> Restart session\n",
        "#   3. Run all remaining sections in order\n",
        "# ============================================================\n",
        "\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# SECTION 1 — Check GPU\n",
        "# ────────────────────────────────────────────────────────────\n",
        "import torch\n",
        "print(f\"CUDA available : {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU            : {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory     : {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    raise SystemError(\"No GPU! Go to Runtime > Change runtime type > T4 GPU\")\n",
        "\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# SECTION 2 — Install Dependencies\n",
        "# *** RESTART SESSION after this cell ***\n",
        "# ────────────────────────────────────────────────────────────\n",
        "import subprocess, sys\n",
        "\n",
        "cmds = [\n",
        "    \"pip install -q transformers==4.49.0\",\n",
        "    \"pip install -q qwen-vl-utils\",\n",
        "    \"pip install -q peft accelerate bitsandbytes\",\n",
        "    \"pip install -q datasets pillow torchvision\",\n",
        "]\n",
        "for cmd in cmds:\n",
        "    subprocess.run(cmd.split(), check=True)\n",
        "\n",
        "print(\"\\nDone! >>> Runtime -> Restart session, then run from Section 3 <<<\")\n",
        "\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# SECTION 3 — HF Login (optional)\n",
        "# ────────────────────────────────────────────────────────────\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    from huggingface_hub import login\n",
        "    token = userdata.get(\"HF_TOKEN\")\n",
        "    if token:\n",
        "        login(token=token)\n",
        "        print(\"Logged in\")\n",
        "    else:\n",
        "        print(\"Anonymous access\")\n",
        "except Exception:\n",
        "    print(\"Anonymous access\")\n",
        "\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# SECTION 4 — Load and Flatten Dataset\n",
        "# ────────────────────────────────────────────────────────────\n",
        "from datasets import load_dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import json, ast, torch\n",
        "\n",
        "print(\"Loading dataset ...\")\n",
        "dataset = load_dataset(\n",
        "    \"sujet-ai/Sujet-Finance-QA-Vision-100k\",\n",
        "    split=\"train\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "print(f\"Loaded {len(dataset):,} documents\")\n",
        "print(\"Raw qa_pairs sample:\", str(dataset[0][\"qa_pairs\"])[:300])\n",
        "\n",
        "\n",
        "def parse_qa_pairs(qa_str):\n",
        "    if not qa_str:\n",
        "        return []\n",
        "    try:\n",
        "        pairs = json.loads(qa_str)\n",
        "    except Exception:\n",
        "        try:\n",
        "            pairs = ast.literal_eval(qa_str)\n",
        "        except Exception:\n",
        "            return []\n",
        "    if isinstance(pairs, dict):\n",
        "        pairs = [pairs]\n",
        "    if not isinstance(pairs, list):\n",
        "        return []\n",
        "    results = []\n",
        "    for p in pairs:\n",
        "        if not isinstance(p, dict):\n",
        "            continue\n",
        "        q = (p.get(\"question\") or p.get(\"Q\") or p.get(\"q\") or \"\").strip()\n",
        "        a = (p.get(\"answer\")   or p.get(\"A\") or p.get(\"a\") or \"\").strip()\n",
        "        if q and a:\n",
        "            results.append((q, a))\n",
        "    return results\n",
        "\n",
        "\n",
        "flat_images, flat_questions, flat_answers = [], [], []\n",
        "for row in dataset:\n",
        "    for q, a in parse_qa_pairs(row[\"qa_pairs\"]):\n",
        "        flat_images.append(row[\"image\"])\n",
        "        flat_questions.append(q)\n",
        "        flat_answers.append(a)\n",
        "\n",
        "print(f\"Total QA pairs: {len(flat_questions):,}\")\n",
        "print(f\"Sample Q: {flat_questions[0][:120]}\")\n",
        "print(f\"Sample A: {flat_answers[0][:120]}\")\n",
        "\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# SECTION 5 — Load Model & Processor\n",
        "# ────────────────────────────────────────────────────────────\n",
        "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
        "\n",
        "MODEL_NAME = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
        "DEVICE     = \"cuda\"\n",
        "DTYPE      = torch.bfloat16\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    min_pixels=128 * 28 * 28,\n",
        "    max_pixels=256 * 28 * 28,  # smaller = less GPU memory per image\n",
        ")\n",
        "\n",
        "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=DTYPE,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "print(f\"Params: {sum(p.numel() for p in model.parameters())/1e9:.2f}B\")\n",
        "print(f\"GPU   : {torch.cuda.memory_allocated()/1e9:.2f} GB used\")\n",
        "\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# SECTION 6 — Apply LoRA\n",
        "# ────────────────────────────────────────────────────────────\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "model = get_peft_model(model, LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=\"all-linear\",\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "))\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# SECTION 7 — Dataset & DataLoader\n",
        "# ────────────────────────────────────────────────────────────\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import random\n",
        "\n",
        "BATCH_SIZE    = 1        # 1 is safest on T4; increase to 2 if no OOM\n",
        "TRAIN_SAMPLES = 3000\n",
        "VAL_SAMPLES   = 200\n",
        "SEED          = 42\n",
        "MAX_SEQ_LEN   = 768\n",
        "\n",
        "\n",
        "class FinanceVQADataset(Dataset):\n",
        "    def __init__(self, images, questions, answers):\n",
        "        self.images    = images\n",
        "        self.questions = questions\n",
        "        self.answers   = answers\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.images[idx]\n",
        "        if not isinstance(img, Image.Image):\n",
        "            img = Image.fromarray(img)\n",
        "        return {\n",
        "            \"image\":    img.convert(\"RGB\"),\n",
        "            \"question\": self.questions[idx],\n",
        "            \"answer\":   self.answers[idx],\n",
        "        }\n",
        "\n",
        "\n",
        "def make_messages(question, answer=None):\n",
        "    \"\"\"\n",
        "    Build Qwen2-VL message list.\n",
        "    If answer is provided, append it as the assistant turn (for training).\n",
        "    \"\"\"\n",
        "    user_content = [\n",
        "        {\"type\": \"image\", \"image\": \"placeholder\"},   # placeholder replaced below\n",
        "        {\"type\": \"text\",  \"text\": (\n",
        "            \"You are a financial document expert. \"\n",
        "            \"Answer the question based on the document image.\\n\\n\"\n",
        "            f\"Question: {question}\"\n",
        "        )},\n",
        "    ]\n",
        "    msgs = [{\"role\": \"user\", \"content\": user_content}]\n",
        "    if answer is not None:\n",
        "        msgs.append({\"role\": \"assistant\", \"content\": answer})\n",
        "    return msgs\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Key insight: we must let the processor build input_ids and pixel_values\n",
        "    together from the SAME messages so the image token count matches.\n",
        "    We then mask the prompt part of labels with -100.\n",
        "    \"\"\"\n",
        "    all_input_ids      = []\n",
        "    all_attention_mask = []\n",
        "    all_labels         = []\n",
        "    all_pixel_values   = []\n",
        "    all_image_grid_thw = []\n",
        "\n",
        "    for item in batch:\n",
        "        image    = item[\"image\"]\n",
        "        question = item[\"question\"]\n",
        "        answer   = item[\"answer\"]\n",
        "\n",
        "        # ── Build full conversation (user + assistant) ──────────\n",
        "        # We use qwen_vl_utils to properly insert the image\n",
        "        from qwen_vl_utils import process_vision_info\n",
        "\n",
        "        full_messages = [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"image\", \"image\": image},\n",
        "                    {\"type\": \"text\",  \"text\": (\n",
        "                        \"You are a financial document expert. \"\n",
        "                        \"Answer the question based on the document image.\\n\\n\"\n",
        "                        f\"Question: {question}\"\n",
        "                    )},\n",
        "                ],\n",
        "            },\n",
        "            {\"role\": \"assistant\", \"content\": answer},\n",
        "        ]\n",
        "\n",
        "        prompt_messages = full_messages[:-1]  # user turn only\n",
        "\n",
        "        # Apply chat template to full text and prompt only\n",
        "        full_text   = processor.apply_chat_template(full_messages,   tokenize=False, add_generation_prompt=False)\n",
        "        prompt_text = processor.apply_chat_template(prompt_messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "        # Extract image inputs using qwen_vl_utils\n",
        "        image_inputs, _ = process_vision_info(full_messages)\n",
        "\n",
        "        # Process full sequence with image\n",
        "        enc = processor(\n",
        "            text=[full_text],\n",
        "            images=image_inputs,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=False,\n",
        "            truncation=True,\n",
        "            max_length=MAX_SEQ_LEN,\n",
        "        )\n",
        "\n",
        "        # Process prompt only (no image needed for length calculation)\n",
        "        # We tokenize just the text to find where the answer starts\n",
        "        prompt_ids = processor.tokenizer(\n",
        "            prompt_text,\n",
        "            return_tensors=\"pt\",\n",
        "            add_special_tokens=False,\n",
        "        ).input_ids\n",
        "\n",
        "        input_ids = enc[\"input_ids\"][0]       # shape: (seq_len,)\n",
        "        labels    = input_ids.clone()\n",
        "\n",
        "        # Mask everything up to (but not including) the answer tokens\n",
        "        prompt_len = min(len(prompt_ids[0]), len(labels))\n",
        "        labels[:prompt_len] = -100            # don't compute loss on prompt\n",
        "\n",
        "        all_input_ids.append(input_ids)\n",
        "        all_attention_mask.append(enc[\"attention_mask\"][0])\n",
        "        all_labels.append(labels)\n",
        "        all_pixel_values.append(enc[\"pixel_values\"])\n",
        "        all_image_grid_thw.append(enc[\"image_grid_thw\"])\n",
        "\n",
        "    # ── Pad to longest sequence in batch ───────────────────────\n",
        "    max_len = max(x.shape[0] for x in all_input_ids)\n",
        "    pad_id  = processor.tokenizer.pad_token_id or 0\n",
        "\n",
        "    batch_input_ids = torch.full((len(batch), max_len), pad_id,  dtype=torch.long)\n",
        "    batch_attn_mask = torch.zeros((len(batch), max_len),          dtype=torch.long)\n",
        "    batch_labels    = torch.full((len(batch), max_len), -100,     dtype=torch.long)\n",
        "\n",
        "    for i, (ids, mask, lbls) in enumerate(zip(all_input_ids, all_attention_mask, all_labels)):\n",
        "        L = ids.shape[0]\n",
        "        batch_input_ids[i, :L] = ids\n",
        "        batch_attn_mask[i, :L] = mask\n",
        "        batch_labels[i, :L]    = lbls\n",
        "\n",
        "    pixel_values   = torch.cat(all_pixel_values,   dim=0)\n",
        "    image_grid_thw = torch.cat(all_image_grid_thw, dim=0)\n",
        "\n",
        "    return {\n",
        "        \"input_ids\":      batch_input_ids.to(DEVICE),\n",
        "        \"attention_mask\": batch_attn_mask.to(DEVICE),\n",
        "        \"labels\":         batch_labels.to(DEVICE),\n",
        "        \"pixel_values\":   pixel_values.to(DEVICE, dtype=DTYPE),\n",
        "        \"image_grid_thw\": image_grid_thw.to(DEVICE),\n",
        "    }\n",
        "\n",
        "\n",
        "# Split\n",
        "indices = list(range(len(flat_images)))\n",
        "random.seed(SEED)\n",
        "random.shuffle(indices)\n",
        "\n",
        "n_val   = min(VAL_SAMPLES, int(len(indices) * 0.1))\n",
        "n_train = min(TRAIN_SAMPLES, len(indices) - n_val)\n",
        "tr_idx  = indices[:n_train]\n",
        "va_idx  = indices[n_train:n_train + n_val]\n",
        "\n",
        "train_ds = FinanceVQADataset([flat_images[i] for i in tr_idx],\n",
        "                              [flat_questions[i] for i in tr_idx],\n",
        "                              [flat_answers[i]   for i in tr_idx])\n",
        "val_ds   = FinanceVQADataset([flat_images[i] for i in va_idx],\n",
        "                              [flat_questions[i] for i in va_idx],\n",
        "                              [flat_answers[i]   for i in va_idx])\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                          collate_fn=collate_fn, num_workers=0)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False,\n",
        "                          collate_fn=collate_fn, num_workers=0)\n",
        "\n",
        "print(f\"Train: {len(train_ds):,} | Val: {len(val_ds):,}\")\n",
        "\n",
        "# Quick sanity check — confirm one batch works before training\n",
        "print(\"\\nRunning sanity check on one batch ...\")\n",
        "test_batch = next(iter(train_loader))\n",
        "with torch.no_grad():\n",
        "    out = model(**test_batch)\n",
        "print(f\"Sanity check passed! Loss = {out.loss.item():.4f}\")\n",
        "del test_batch\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# SECTION 8 — Optimizer & Scheduler\n",
        "# ────────────────────────────────────────────────────────────\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "import os, time\n",
        "\n",
        "NUM_EPOCHS         = 3\n",
        "LEARNING_RATE      = 2e-4\n",
        "WEIGHT_DECAY       = 0.01\n",
        "GRAD_CLIP          = 1.0\n",
        "ACCUMULATION_STEPS = 8\n",
        "LOG_EVERY          = 20\n",
        "SAVE_DIR           = \"/content/finance_vqa_ckpt\"\n",
        "\n",
        "optimizer = AdamW(\n",
        "    [p for p in model.parameters() if p.requires_grad],\n",
        "    lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY,\n",
        ")\n",
        "total_steps = max(1, (len(train_loader) * NUM_EPOCHS) // ACCUMULATION_STEPS)\n",
        "scheduler   = OneCycleLR(optimizer, max_lr=LEARNING_RATE,\n",
        "                         total_steps=total_steps, pct_start=0.1)\n",
        "scaler      = GradScaler(enabled=True)\n",
        "\n",
        "print(f\"Steps: {total_steps} | Effective batch: {BATCH_SIZE * ACCUMULATION_STEPS}\")\n",
        "\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# SECTION 9 — Training Loop\n",
        "# ────────────────────────────────────────────────────────────\n",
        "\n",
        "def evaluate(model, loader, max_batches=15):\n",
        "    model.eval()\n",
        "    total, n = 0.0, 0\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(loader):\n",
        "            if i >= max_batches:\n",
        "                break\n",
        "            try:\n",
        "                with autocast(dtype=torch.bfloat16):\n",
        "                    out = model(**batch)\n",
        "                if out.loss is not None:\n",
        "                    total += out.loss.item(); n += 1\n",
        "            except Exception:\n",
        "                pass\n",
        "    return total / n if n else float(\"inf\")\n",
        "\n",
        "\n",
        "def save_ckpt(model, processor, epoch, step, loss):\n",
        "    p = os.path.join(SAVE_DIR, f\"ep{epoch}_s{step}_l{loss:.3f}\")\n",
        "    os.makedirs(p, exist_ok=True)\n",
        "    model.save_pretrained(p); processor.save_pretrained(p)\n",
        "    print(f\"Saved -> {p}\")\n",
        "    return p\n",
        "\n",
        "\n",
        "train_losses, val_losses, step_log = [], [], []\n",
        "global_step = 0\n",
        "best_val    = float(\"inf\")\n",
        "best_ckpt   = None\n",
        "\n",
        "print(\"=\" * 55)\n",
        "print(\"Finance VQA Fine-Tuning — Qwen2-VL-2B + LoRA\")\n",
        "print(\"=\" * 55)\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    ep_loss, t0 = 0.0, time.time()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    for bi, batch in enumerate(train_loader):\n",
        "        try:\n",
        "            with autocast(dtype=torch.bfloat16):\n",
        "                loss = model(**batch).loss / ACCUMULATION_STEPS\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            if (bi + 1) % ACCUMULATION_STEPS == 0:\n",
        "                scaler.unscale_(optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(\n",
        "                    [p for p in model.parameters() if p.requires_grad], GRAD_CLIP)\n",
        "                scaler.step(optimizer); scaler.update()\n",
        "                scheduler.step(); optimizer.zero_grad()\n",
        "                global_step += 1\n",
        "\n",
        "            ep_loss += loss.item() * ACCUMULATION_STEPS\n",
        "\n",
        "            if (bi + 1) % LOG_EVERY == 0:\n",
        "                avg = ep_loss / (bi + 1)\n",
        "                print(f\"[Ep{epoch+1}/{NUM_EPOCHS}] \"\n",
        "                      f\"B{bi+1}/{len(train_loader)} | \"\n",
        "                      f\"Loss {avg:.4f} | \"\n",
        "                      f\"GPU {torch.cuda.memory_allocated()/1e9:.1f}GB | \"\n",
        "                      f\"{time.time()-t0:.0f}s\")\n",
        "                train_losses.append(avg); step_log.append(global_step)\n",
        "\n",
        "        except RuntimeError as e:\n",
        "            if \"out of memory\" in str(e).lower():\n",
        "                print(f\"OOM at batch {bi} — skipping\")\n",
        "                torch.cuda.empty_cache(); optimizer.zero_grad()\n",
        "            else:\n",
        "                raise\n",
        "\n",
        "    vl = evaluate(model, val_loader)\n",
        "    at = ep_loss / max(1, len(train_loader))\n",
        "    print(f\"\\nEpoch {epoch+1}: train={at:.4f} val={vl:.4f} \"\n",
        "          f\"({(time.time()-t0)/60:.1f}min)\\n\")\n",
        "    val_losses.append(vl)\n",
        "    if vl < best_val:\n",
        "        best_val  = vl\n",
        "        best_ckpt = save_ckpt(model, processor, epoch+1, global_step, vl)\n",
        "    model.train()\n",
        "\n",
        "print(f\"Done! Best val loss: {best_val:.4f}  ckpt: {best_ckpt}\")\n",
        "\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# SECTION 10 — Plot Curves\n",
        "# ────────────────────────────────────────────────────────────\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13, 4))\n",
        "if train_losses:\n",
        "    ax1.plot(step_log, train_losses, \"b-\"); ax1.set_title(\"Train Loss\"); ax1.grid(True, alpha=0.3)\n",
        "if val_losses:\n",
        "    ax2.plot(range(1, len(val_losses)+1), val_losses, \"r-o\"); ax2.set_title(\"Val Loss\"); ax2.grid(True, alpha=0.3)\n",
        "plt.suptitle(\"Finance VQA Fine-Tuning\"); plt.tight_layout()\n",
        "plt.savefig(\"/content/curves.png\", dpi=130); plt.show()\n",
        "\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# SECTION 11 — Inference\n",
        "# ────────────────────────────────────────────────────────────\n",
        "from qwen_vl_utils import process_vision_info\n",
        "\n",
        "def run_inference(model, processor, image, question, max_new_tokens=128):\n",
        "    model.eval()\n",
        "    if not isinstance(image, Image.Image):\n",
        "        image = Image.fromarray(image)\n",
        "    image = image.convert(\"RGB\")\n",
        "\n",
        "    messages = [{\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\", \"image\": image},\n",
        "            {\"type\": \"text\",  \"text\": f\"You are a financial document expert.\\nQuestion: {question}\"},\n",
        "        ],\n",
        "    }]\n",
        "    prompt       = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    image_inputs, _ = process_vision_info(messages)\n",
        "\n",
        "    inputs = processor(text=[prompt], images=image_inputs,\n",
        "                       return_tensors=\"pt\", padding=True)\n",
        "    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
        "    if \"pixel_values\" in inputs:\n",
        "        inputs[\"pixel_values\"] = inputs[\"pixel_values\"].to(dtype=DTYPE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out_ids = model.generate(**inputs, max_new_tokens=max_new_tokens,\n",
        "                                 do_sample=False, repetition_penalty=1.1)\n",
        "\n",
        "    answer = processor.tokenizer.decode(\n",
        "        out_ids[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True\n",
        "    ).strip()\n",
        "    return answer\n",
        "\n",
        "\n",
        "print(\"Inference on 5 validation samples:\\n\")\n",
        "for i in range(min(5, len(val_ds))):\n",
        "    item = val_ds[i]\n",
        "    pred = run_inference(model, processor, item[\"image\"], item[\"question\"])\n",
        "    print(f\"--- {i+1} ---\")\n",
        "    print(f\"Q   : {item['question'][:180]}\")\n",
        "    print(f\"GT  : {item['answer'][:180]}\")\n",
        "    print(f\"PRED: {pred[:180]}\\n\")\n",
        "\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# SECTION 12 — Metrics\n",
        "# ────────────────────────────────────────────────────────────\n",
        "import re, numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "def norm(s):\n",
        "    s = re.sub(r\"[^\\w\\s]\", \"\", str(s).lower().strip())\n",
        "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
        "\n",
        "def em(p, g):   return int(norm(p) == norm(g))\n",
        "def f1(p, g):\n",
        "    pt = norm(p).split(); gt = norm(g).split()\n",
        "    if not pt or not gt: return 0.0\n",
        "    c = sum((Counter(pt) & Counter(gt)).values())\n",
        "    if c == 0: return 0.0\n",
        "    return 2*c/(len(pt)+len(gt))\n",
        "\n",
        "ems, f1s = [], []\n",
        "N = min(50, len(val_ds))\n",
        "for i in range(N):\n",
        "    item = val_ds[i]\n",
        "    pred = run_inference(model, processor, item[\"image\"], item[\"question\"])\n",
        "    ems.append(em(pred, item[\"answer\"]))\n",
        "    f1s.append(f1(pred, item[\"answer\"]))\n",
        "    if (i+1) % 10 == 0: print(f\"{i+1}/{N} evaluated\")\n",
        "\n",
        "print(f\"\\nExact Match : {np.mean(ems)*100:.1f}%\")\n",
        "print(f\"Token F1    : {np.mean(f1s)*100:.1f}%\")\n",
        "\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# SECTION 13 — Save & Download\n",
        "# ────────────────────────────────────────────────────────────\n",
        "import subprocess\n",
        "from google.colab import files\n",
        "\n",
        "OUT = \"/content/finance_vqa_final\"\n",
        "os.makedirs(OUT, exist_ok=True)\n",
        "model.save_pretrained(OUT); processor.save_pretrained(OUT)\n",
        "subprocess.run([\"zip\", \"-r\", \"-q\", \"/content/finance_vqa.zip\", OUT])\n",
        "files.download(\"/content/finance_vqa.zip\")\n",
        "print(\"Download started!\")\n",
        "\n",
        "\n",
        "# ────────────────────────────────────────────────────────────\n",
        "# SECTION 14 — Reload Snippet\n",
        "# ────────────────────────────────────────────────────────────\n",
        "print(\"\"\"\n",
        "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "\n",
        "ADAPTER = \"/content/finance_vqa_final\"\n",
        "BASE    = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(ADAPTER)\n",
        "base  = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "    BASE, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
        "model = PeftModel.from_pretrained(base, ADAPTER)\n",
        "model = model.merge_and_unload()\n",
        "model.eval()\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "6eloj2SigKoH",
        "outputId": "a5cb9556-40e0-4d53-9efc-d1f149c2d8c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available : True\n",
            "GPU            : NVIDIA A100-SXM4-80GB\n",
            "GPU Memory     : 85.09 GB\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2593646780.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m ]\n\u001b[1;32m     38\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcmd\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcmds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nDone! >>> Runtime -> Restart session, then run from Section 3 <<<\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTimeoutExpired\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1199\u001b[0m                 \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1201\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1202\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1203\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/subprocess.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1262\u001b[0m             \u001b[0mendtime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1263\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1264\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1265\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# https://bugs.python.org/issue25942\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/subprocess.py\u001b[0m in \u001b[0;36m_wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   2051\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2052\u001b[0m                             \u001b[0;32mbreak\u001b[0m  \u001b[0;31m# Another thread waited.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2053\u001b[0;31m                         \u001b[0;34m(\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2054\u001b[0m                         \u001b[0;31m# Check the pid and loop as waitpid has been known to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2055\u001b[0m                         \u001b[0;31m# return 0 even without WNOHANG in odd situations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/subprocess.py\u001b[0m in \u001b[0;36m_try_wait\u001b[0;34m(self, wait_flags)\u001b[0m\n\u001b[1;32m   2009\u001b[0m             \u001b[0;34m\"\"\"All callers to this function MUST hold self._waitpid_lock.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2010\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2011\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait_flags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2012\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mChildProcessError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2013\u001b[0m                 \u001b[0;31m# This happens if SIGCLD is set to be ignored or waiting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}