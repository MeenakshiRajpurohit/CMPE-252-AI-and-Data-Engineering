{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "private_outputs": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MeenakshiRajpurohit/CMPE-252-AI-and-Data-Engineering/blob/main/VQA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eba085c3"
      },
      "source": [
        "#pip install --upgrade transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANig36-ItRmA"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q datasets torch torchvision\n",
        "!pip install -q accelerate bitsandbytes peft\n",
        "!pip install -q sentencepiece pillow tqdm\n",
        "!pip install -q evaluate rouge-score\n",
        "!pip install -q matplotlib seaborn\n",
        "\n",
        "print(\"✓ All packages installed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from transformers import (\n",
        "    AutoProcessor,\n",
        "    #AutoModelForVision2Seq,\n",
        "    VisionEncoderDecoderModel,\n",
        "    DonutProcessor,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        ")\n",
        "\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    prepare_model_for_kbit_training,\n",
        ")\n",
        "\n",
        "from datasets import Dataset, load_dataset\n",
        "\n",
        "# Check GPU\n",
        "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
      ],
      "metadata": {
        "id": "AifjDTi4tkOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "from datasets import load_dataset\n",
        "\n",
        "try:\n",
        "    # Try loading from HuggingFace\n",
        "    dataset = load_dataset(\"sujet-ai/Sujet-Finance-QA-Vision-100k\", split=\"train\")\n",
        "    print(f\"✓ Loaded {len(dataset)} samples from HuggingFace\")\n",
        "except:\n",
        "    # Fallback: Load via API\n",
        "    print(\"Loading via API...\")\n",
        "    url = \"https://datasets-server.huggingface.co/rows?dataset=sujet-ai%2FSujet-Finance-QA-Vision-100k&config=default&split=train&offset=0&length=1000\"\n",
        "    response = requests.get(url)\n",
        "    data = response.json()\n",
        "    rows = [row['row'] for row in data.get('rows', [])]\n",
        "    dataset = Dataset.from_list(rows)\n",
        "    print(f\"✓ Loaded {len(dataset)} samples via API\")"
      ],
      "metadata": {
        "id": "zXXk_snnvkTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Explore dataset structure\n",
        "print(\"Dataset columns:\", dataset.column_names)\n",
        "print(\"\\nFirst example:\")\n",
        "print(json.dumps(dataset[0], indent=2, default=str))"
      ],
      "metadata": {
        "id": "BcCVAhhqv3Er"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize sample\n",
        "def visualize_sample(example):\n",
        "    \"\"\"Display a sample from the dataset\"\"\"\n",
        "    # Load image\n",
        "    if isinstance(example['image'], str):\n",
        "        response = requests.get(example['image'])\n",
        "        image = Image.open(BytesIO(response.content))\n",
        "    else:\n",
        "        image = example['image']\n",
        "\n",
        "    # Display\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.imshow(image)\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"Question: {example.get('question', example.get('query', 'N/A'))}\")\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Answer: {example.get('answer', example.get('response', 'N/A'))}\")\n",
        "\n",
        "# Show first 3 samples\n",
        "for i in range(min(3, len(dataset))):\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Sample {i+1}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    visualize_sample(dataset[i])"
      ],
      "metadata": {
        "id": "KGupMtRQwGe2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split dataset\n",
        "train_test_split = dataset.train_test_split(test_size=0.1, seed=42)\n",
        "train_dataset = train_test_split['train']\n",
        "test_dataset = train_test_split['test']\n",
        "\n",
        "# Further split test into validation and test\n",
        "val_test_split = test_dataset.train_test_split(test_size=0.5, seed=42)\n",
        "val_dataset = val_test_split['train']\n",
        "test_dataset = val_test_split['test']\n",
        "\n",
        "print(f\"Train: {len(train_dataset)} samples\")\n",
        "print(f\"Validation: {len(val_dataset)} samples\")\n",
        "print(f\"Test: {len(test_dataset)} samples\")"
      ],
      "metadata": {
        "id": "v2enlP1AwVhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training configuration\n",
        "CONFIG = {\n",
        "    # Model selection (change this to train different models)\n",
        "    'model_type': 'donut',  # Options: 'donut', 'blip', 'pix2struct'\n",
        "\n",
        "    # Data configuration\n",
        "    'num_train_samples': 500,  # Reduce for testing, increase for full training\n",
        "    'num_val_samples': 100,\n",
        "\n",
        "    # Training hyperparameters\n",
        "    'epochs': 3,\n",
        "    'batch_size': 2,  # Reduce if OOM\n",
        "    'gradient_accumulation_steps': 8,\n",
        "    'learning_rate': 5e-5,\n",
        "    'warmup_steps': 100,\n",
        "\n",
        "    # QLoRA hyperparameters\n",
        "    'lora_r': 16,\n",
        "    'lora_alpha': 32,\n",
        "    'lora_dropout': 0.05,\n",
        "\n",
        "    # Output\n",
        "    'output_dir': f'./finance-vqa-qlora',\n",
        "}\n",
        "\n",
        "print(\"Configuration:\")\n",
        "print(json.dumps(CONFIG, indent=2))"
      ],
      "metadata": {
        "id": "BZcTF9nEwgSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select processor based on model type\n",
        "if CONFIG['model_type'] == 'donut':\n",
        "    model_name = \"naver-clova-ix/donut-base\"\n",
        "    processor = DonutProcessor.from_pretrained(model_name)\n",
        "\n",
        "elif CONFIG['model_type'] == 'blip':\n",
        "    model_name = \"Salesforce/blip-vqa-base\"\n",
        "    processor = AutoProcessor.from_pretrained(model_name)\n",
        "\n",
        "elif CONFIG['model_type'] == 'pix2struct':\n",
        "    model_name = \"google/pix2struct-docvqa-base\"\n",
        "    processor = AutoProcessor.from_pretrained(model_name)\n",
        "\n",
        "print(f\"✓ Loaded processor for {model_name}\")"
      ],
      "metadata": {
        "id": "7Bx-DOZKw3RK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_example_donut(example):\n",
        "    \"\"\"Process example for Donut model\"\"\"\n",
        "    # Load image\n",
        "    if isinstance(example['image'], str):\n",
        "        response = requests.get(example['image'])\n",
        "        image = Image.open(BytesIO(response.content)).convert('RGB')\n",
        "    else:\n",
        "        image = example['image']\n",
        "\n",
        "    question = example.get('question', example.get('query', ''))\n",
        "    answer = example.get('answer', example.get('response', ''))\n",
        "\n",
        "    # Format for Donut\n",
        "    prompt = f\"<s_docvqa><s_question>{question}</s_question><s_answer>\"\n",
        "    target = f\"{answer}</s_answer></s_docvqa>\"\n",
        "\n",
        "    # Encode\n",
        "    pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
        "\n",
        "    decoder_input_ids = processor.tokenizer(\n",
        "        prompt,\n",
        "        add_special_tokens=False,\n",
        "        max_length=512,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).input_ids\n",
        "\n",
        "    labels = processor.tokenizer(\n",
        "        target,\n",
        "        add_special_tokens=False,\n",
        "        max_length=512,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).input_ids\n",
        "\n",
        "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
        "\n",
        "    return {\n",
        "        'pixel_values': pixel_values.squeeze(),\n",
        "        'decoder_input_ids': decoder_input_ids.squeeze(),\n",
        "        'labels': labels.squeeze()\n",
        "    }\n",
        "\n",
        "def process_example_blip(example):\n",
        "    \"\"\"Process example for BLIP model\"\"\"\n",
        "    if isinstance(example['image'], str):\n",
        "        response = requests.get(example['image'])\n",
        "        image = Image.open(BytesIO(response.content)).convert('RGB')\n",
        "    else:\n",
        "        image = example['image']\n",
        "\n",
        "    question = example.get('question', example.get('query', ''))\n",
        "    answer = example.get('answer', example.get('response', ''))\n",
        "\n",
        "    encoding = processor(\n",
        "        images=image,\n",
        "        text=question,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=128,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    labels = processor.tokenizer(\n",
        "        answer,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=128,\n",
        "        return_tensors=\"pt\"\n",
        "    ).input_ids\n",
        "\n",
        "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
        "\n",
        "    return {\n",
        "        'pixel_values': encoding['pixel_values'].squeeze(),\n",
        "        'input_ids': encoding['input_ids'].squeeze(),\n",
        "        'attention_mask': encoding['attention_mask'].squeeze(),\n",
        "        'labels': labels.squeeze()\n",
        "    }\n",
        "\n",
        "def process_example_pix2struct(example):\n",
        "    \"\"\"Process example for Pix2Struct model\"\"\"\n",
        "    if isinstance(example['image'], str):\n",
        "        response = requests.get(example['image'])\n",
        "        image = Image.open(BytesIO(response.content)).convert('RGB')\n",
        "    else:\n",
        "        image = example['image']\n",
        "\n",
        "    question = example.get('question', example.get('query', ''))\n",
        "    answer = example.get('answer', example.get('response', ''))\n",
        "\n",
        "    encoding = processor(\n",
        "        images=image,\n",
        "        text=question,\n",
        "        return_tensors=\"pt\",\n",
        "        max_patches=2048\n",
        "    )\n",
        "\n",
        "    labels = processor.tokenizer(\n",
        "        answer,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).input_ids\n",
        "\n",
        "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
        "\n",
        "    return {\n",
        "        'flattened_patches': encoding['flattened_patches'].squeeze(),\n",
        "        'attention_mask': encoding['attention_mask'].squeeze(),\n",
        "        'labels': labels.squeeze()\n",
        "    }\n",
        "\n",
        "# Select processing function\n",
        "if CONFIG['model_type'] == 'donut':\n",
        "    process_fn = process_example_donut\n",
        "elif CONFIG['model_type'] == 'blip':\n",
        "    process_fn = process_example_blip\n",
        "else:\n",
        "    process_fn = process_example_pix2struct"
      ],
      "metadata": {
        "id": "5aM8y_XrxFQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "this below one cell is extra addiytion\n"
      ],
      "metadata": {
        "id": "Gw4K4thTemrj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use .map() instead of a for-loop to keep RAM usage near zero\n",
        "print(\"Processing training data...\")\n",
        "train_data = train_dataset.select(range(CONFIG['num_train_samples'])).map(\n",
        "    process_fn,\n",
        "    remove_columns=train_dataset.column_names, # Clears old data to save space\n",
        "    desc=\"Mapping train data\"\n",
        ")\n",
        "\n",
        "print(\"\\nProcessing validation data...\")\n",
        "val_data = val_dataset.select(range(CONFIG['num_val_samples'])).map(\n",
        "    process_fn,\n",
        "    remove_columns=val_dataset.column_names,\n",
        "    desc=\"Mapping val data\"\n",
        ")\n",
        "\n",
        "print(f\"\\n✓ Processed {len(train_data)} training samples\")\n",
        "print(f\"✓ Processed {len(val_data)} validation samples\")"
      ],
      "metadata": {
        "id": "gHeIa76CeSgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Process datasets\n",
        "print(\"Processing training data...\")\n",
        "processed_train = []\n",
        "for example in tqdm(train_dataset.select(range(CONFIG['num_train_samples']))):\n",
        "    try:\n",
        "        processed_train.append(process_fn(example))\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        continue\n",
        "\n",
        "print(\"\\nProcessing validation data...\")\n",
        "processed_val = []\n",
        "for example in tqdm(val_dataset.select(range(CONFIG['num_val_samples']))):\n",
        "    try:\n",
        "        processed_val.append(process_fn(example))\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        continue\n",
        "\n",
        "train_data = Dataset.from_list(processed_train)\n",
        "val_data = Dataset.from_list(processed_val)\n",
        "\n",
        "print(f\"\\n✓ Processed {len(train_data)} training samples\")\n",
        "print(f\"✓ Processed {len(val_data)} validation samples\")"
      ],
      "metadata": {
        "id": "rLnYtwdfxNkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BitsAndBytes config for 4-bit quantization\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True\n",
        ")\n",
        "\n",
        "# LoRA config\n",
        "lora_config = LoraConfig(\n",
        "    r=CONFIG['lora_r'],\n",
        "    lora_alpha=CONFIG['lora_alpha'],\n",
        "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
        "    lora_dropout=CONFIG['lora_dropout'],\n",
        "    bias=\"none\",\n",
        "    task_type=\"SEQ_2_SEQ_LM\"\n",
        ")\n",
        "\n",
        "print(\"✓ QLoRA configuration ready\")"
      ],
      "metadata": {
        "id": "OjcFuE0q2KZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model\n",
        "print(f\"Loading {model_name} with 4-bit quantization...\")\n",
        "\n",
        "if CONFIG['model_type'] == 'donut':\n",
        "    model = VisionEncoderDecoderModel.from_pretrained(\n",
        "        model_name,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "else:\n",
        "    model = AutoModelForVision2Seq.from_pretrained(\n",
        "        model_name,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "# Prepare for k-bit training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# Add LoRA adapters\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Print trainable parameters\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "print(\"✓ Model loaded successfully!\")"
      ],
      "metadata": {
        "id": "Z0rItHZs2O_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=CONFIG['output_dir'],\n",
        "    num_train_epochs=CONFIG['epochs'],\n",
        "    per_device_train_batch_size=CONFIG['batch_size'],\n",
        "    per_device_eval_batch_size=CONFIG['batch_size'],\n",
        "    gradient_accumulation_steps=CONFIG['gradient_accumulation_steps'],\n",
        "    learning_rate=CONFIG['learning_rate'],\n",
        "    weight_decay=0.01,\n",
        "    warmup_steps=CONFIG['warmup_steps'],\n",
        "    logging_steps=10,\n",
        "    save_steps=200,\n",
        "    eval_steps=200,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    save_total_limit=2,\n",
        "    fp16=True,\n",
        "    dataloader_num_workers=2,\n",
        "    remove_unused_columns=False,\n",
        "    report_to=\"none\",\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=val_data,\n",
        ")\n",
        "\n",
        "print(\"✓ Trainer initialized\")"
      ],
      "metadata": {
        "id": "JYlQR1Tl2aKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start training\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(f\"STARTING TRAINING: {CONFIG['model_type'].upper()}\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\n✓ Training complete!\")"
      ],
      "metadata": {
        "id": "9uGKfR0J2lVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model\n",
        "trainer.save_model(CONFIG['output_dir'])\n",
        "processor.save_pretrained(CONFIG['output_dir'])\n",
        "\n",
        "print(f\"✓ Model saved to {CONFIG['output_dir']}\")"
      ],
      "metadata": {
        "id": "8o7E7Z_92mRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test inference on a sample\n",
        "def test_inference(model, processor, example):\n",
        "    \"\"\"Test model on a single example\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Load image\n",
        "    if isinstance(example['image'], str):\n",
        "        response = requests.get(example['image'])\n",
        "        image = Image.open(BytesIO(response.content)).convert('RGB')\n",
        "    else:\n",
        "        image = example['image']\n",
        "\n",
        "    question = example.get('question', example.get('query', ''))\n",
        "\n",
        "    if CONFIG['model_type'] == 'donut':\n",
        "        prompt = f\"<s_docvqa><s_question>{question}</s_question><s_answer>\"\n",
        "        pixel_values = processor(image, return_tensors=\"pt\").pixel_values.to(model.device)\n",
        "        decoder_input_ids = processor.tokenizer(\n",
        "            prompt,\n",
        "            add_special_tokens=False,\n",
        "            return_tensors=\"pt\"\n",
        "        ).input_ids.to(model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                pixel_values,\n",
        "                decoder_input_ids=decoder_input_ids,\n",
        "                max_length=512,\n",
        "                early_stopping=True,\n",
        "            )\n",
        "\n",
        "        answer = processor.batch_decode(outputs)[0]\n",
        "        answer = answer.split(\"<s_answer>\")[-1].split(\"</s_answer>\")[0].strip()\n",
        "\n",
        "    else:\n",
        "        inputs = processor(\n",
        "            images=image,\n",
        "            text=question,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(**inputs, max_length=128)\n",
        "\n",
        "        answer = processor.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    return answer\n",
        "\n",
        "# Test on first 5 samples\n",
        "print(\"Testing inference on sample data...\\n\")\n",
        "for i in range(min(5, len(test_dataset))):\n",
        "    example = test_dataset[i]\n",
        "    prediction = test_inference(model, processor, example)\n",
        "    ground_truth = example.get('answer', example.get('response', ''))\n",
        "\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"Sample {i+1}\")\n",
        "    print(f\"Question: {example.get('question', example.get('query', ''))}\")\n",
        "    print(f\"Predicted: {prediction}\")\n",
        "    print(f\"Ground Truth: {ground_truth}\")\n",
        "    print(f\"Match: {prediction.strip().lower() == ground_truth.strip().lower()}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "vNyeX3su2o6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Comprehensive evaluation\n",
        "def evaluate_model(model, processor, test_data, num_samples=100):\n",
        "    \"\"\"Evaluate model on test set\"\"\"\n",
        "    predictions = []\n",
        "    references = []\n",
        "\n",
        "    print(f\"Evaluating on {num_samples} samples...\\n\")\n",
        "\n",
        "    for example in tqdm(test_data.select(range(min(num_samples, len(test_data))))):\n",
        "        try:\n",
        "            pred = test_inference(model, processor, example)\n",
        "            ref = example.get('answer', example.get('response', ''))\n",
        "\n",
        "            predictions.append(pred)\n",
        "            references.append(ref)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    # Calculate metrics\n",
        "    exact_matches = sum(\n",
        "        p.strip().lower() == r.strip().lower()\n",
        "        for p, r in zip(predictions, references)\n",
        "    )\n",
        "\n",
        "    accuracy = exact_matches / len(predictions) if predictions else 0\n",
        "\n",
        "    # F1 score\n",
        "    f1_scores = []\n",
        "    for pred, ref in zip(predictions, references):\n",
        "        pred_tokens = set(pred.lower().split())\n",
        "        ref_tokens = set(ref.lower().split())\n",
        "\n",
        "        if not ref_tokens:\n",
        "            continue\n",
        "\n",
        "        common = pred_tokens & ref_tokens\n",
        "        if not common:\n",
        "            f1_scores.append(0)\n",
        "            continue\n",
        "\n",
        "        precision = len(common) / len(pred_tokens) if pred_tokens else 0\n",
        "        recall = len(common) / len(ref_tokens)\n",
        "\n",
        "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "        f1_scores.append(f1)\n",
        "\n",
        "    avg_f1 = np.mean(f1_scores) if f1_scores else 0\n",
        "\n",
        "    results = {\n",
        "        'model': CONFIG['model_type'],\n",
        "        'num_samples': len(predictions),\n",
        "        'exact_match': accuracy,\n",
        "        'f1_score': avg_f1,\n",
        "    }\n",
        "\n",
        "    return results, predictions, references\n",
        "\n",
        "# Run evaluation\n",
        "results, preds, refs = evaluate_model(model, processor, test_dataset, num_samples=100)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EVALUATION RESULTS\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Model: {results['model']}\")\n",
        "print(f\"Samples: {results['num_samples']}\")\n",
        "print(f\"Exact Match: {results['exact_match']:.2%}\")\n",
        "print(f\"F1 Score: {results['f1_score']:.4f}\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "vL_9ugg22th6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save evaluation results\n",
        "results_df = pd.DataFrame([results])\n",
        "results_df.to_csv(f\"{CONFIG['output_dir']}/evaluation_results.csv\", index=False)\n",
        "\n",
        "# Save predictions\n",
        "predictions_data = [\n",
        "    {\n",
        "        'prediction': p,\n",
        "        'reference': r,\n",
        "        'correct': p.strip().lower() == r.strip().lower()\n",
        "    }\n",
        "    for p, r in zip(preds, refs)\n",
        "]\n",
        "\n",
        "with open(f\"{CONFIG['output_dir']}/predictions.json\", 'w') as f:\n",
        "    json.dump(predictions_data, f, indent=2)\n",
        "\n",
        "print(f\"✓ Results saved to {CONFIG['output_dir']}/\")"
      ],
      "metadata": {
        "id": "4EVAyXj42y0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: Run experiments with different hyperparameters\n",
        "HYPERPARAMETER_GRID = {\n",
        "    'lora_r': [8, 16, 32],\n",
        "    'lora_alpha': [16, 32, 64],\n",
        "    'learning_rate': [3e-5, 5e-5, 1e-4],\n",
        "}\n",
        "\n",
        "# Uncomment to run grid search (time-consuming)\n",
        "# experiment_results = []\n",
        "#\n",
        "# for r in HYPERPARAMETER_GRID['lora_r']:\n",
        "#     for alpha in HYPERPARAMETER_GRID['lora_alpha']:\n",
        "#         for lr in HYPERPARAMETER_GRID['learning_rate']:\n",
        "#             print(f\"\\nExperiment: r={r}, alpha={alpha}, lr={lr}\")\n",
        "#             # Run training and evaluation\n",
        "#             # Store results\n",
        "#             pass"
      ],
      "metadata": {
        "id": "Enj0aFsq23LC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Option 1: Download as ZIP\n",
        "!zip -r finance-vqa-model.zip {CONFIG['output_dir']}\n",
        "from google.colab import files\n",
        "files.download('finance-vqa-model.zip')\n",
        "\n",
        "# Option 2: Save to Google Drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# !cp -r {CONFIG['output_dir']} /content/drive/MyDrive/"
      ],
      "metadata": {
        "id": "f-bskNC926Xy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_model.py"
      ],
      "metadata": {
        "id": "MEdQFqO7Hzqn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}